{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "from io import StringIO    \n",
    "from skimage import io\n",
    "from skimage import transform as tf\n",
    "from skimage.feature import canny\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.pyplot import imshow\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import os\n",
    "\n",
    "class Image_preprocessing:\n",
    "    def __init__(self, imageObject, num_col):\n",
    "        \"\"\"get an imageobject, return a dataframe\"\"\"\n",
    "        ## preprocess:\n",
    "        bw_image = imageObject.convert('L')\n",
    "        enhanceImage = ImageEnhance.Sharpness(bw_image)\n",
    "        contrast = ImageEnhance.Contrast(enhanceImage.image)\n",
    "        brightness = ImageEnhance.Brightness(contrast.image)\n",
    "        self.imageObject = brightness.image\n",
    "        self.num_col = num_col\n",
    "        # crop the image from the middle. \n",
    "        self.half1, self.half2 = self.image_crop(self.num_col, self.imageObject)\n",
    "        # fix left and right margin:\n",
    "        self.half1= self.sideCrop(self.half1)\n",
    "        self.half2= self.sideCrop(self.half2)\n",
    "        \n",
    "        # Enhance the column image\n",
    "        ## this is similar to prerpocess other than this turns the whole thing\n",
    "        ## black and white. We do it here because the parameters for \n",
    "        ## canny operator and the hough transformation is already determined.  \n",
    "        self.half1 = self.preprocessImage(self.half1)\n",
    "        self.half2 = self.preprocessImage(self.half2)\n",
    "        # Extract OCR data\n",
    "        lineData_1, directoryData_1 = self.ocrInfo(self.half1)\n",
    "        # Identify indented lines\n",
    "        indentList_1, indentColumn_1 = self.indentList(lineData_1)\n",
    "        # Add a space to the end of each extracted word\n",
    "        ocrText_1 = []\n",
    "        for word in range(len(directoryData_1['text'])):\n",
    "            wordSpace = directoryData_1['text'][word] + \" \"\n",
    "            ocrText_1.append(wordSpace)\n",
    "        directoryData_1['text'] = ocrText_1\n",
    "        # Group by Line Number to get lines and their strings\n",
    "        groupedLines_1 = directoryData_1.groupby('line')['text'].apply(lambda x: x.sum()).reset_index()\n",
    "        # Add indent marker to groupedLines\n",
    "        groupedLines_1['Indent'] = indentColumn_1\n",
    "        # Join bifurcated entries together\n",
    "        joinedText_1 = []\n",
    "        for line in range(len(groupedLines_1)):\n",
    "            if groupedLines_1['Indent'][line] == 1:\n",
    "                if line - 1 == -1:\n",
    "                    continue\n",
    "                newText_1 = groupedLines_1['text'][line-1] + groupedLines_1['text'][line]\n",
    "                joinedText_1.pop()\n",
    "                joinedText_1.append(newText_1)\n",
    "            if groupedLines_1['Indent'][line] == 0:\n",
    "                joinedText_1.append(groupedLines_1['text'][line])\n",
    "        # Print final list of entries for the column\n",
    "                # Extract OCR data\n",
    "        lineData_2, directoryData_2 = self.ocrInfo(self.half2)\n",
    "        # Identify indented lines\n",
    "        indentList_2, indentColumn_2 = self.indentList(lineData_2)\n",
    "        # Add a space to the end of each extracted word\n",
    "        ocrText_2 = []\n",
    "        for word in range(len(directoryData_2['text'])):\n",
    "            wordSpace = directoryData_2['text'][word] + \" \"\n",
    "            ocrText_2.append(wordSpace)\n",
    "        directoryData_2['text'] = ocrText_2\n",
    "        # Group by Line Number to get lines and their strings\n",
    "        groupedLines_2 = directoryData_2.groupby('line')['text'].apply(lambda x: x.sum()).reset_index()\n",
    "        # Add indent marker to groupedLines\n",
    "        groupedLines_2['Indent'] = indentColumn_2\n",
    "        # Join bifurcated entries together\n",
    "        joinedText_2 = []\n",
    "        for line in range(len(groupedLines_2)):\n",
    "            if groupedLines_2['Indent'][line] == 1:\n",
    "                if line - 1 == -1:\n",
    "                    continue\n",
    "                newText_2 = groupedLines_2['text'][line-1] + groupedLines_2['text'][line]\n",
    "                joinedText_2.pop()\n",
    "                joinedText_2.append(newText_2)\n",
    "            if groupedLines_2['Indent'][line] == 0:\n",
    "                joinedText_2.append(groupedLines_2['text'][line])\n",
    "                \n",
    "        ## put the list into a dataframe\n",
    "        self.df_1 = pd.DataFrame({'original_line':joinedText_1})\n",
    "        self.df_2 = pd.DataFrame({'original_line':joinedText_2})\n",
    "        self.df = pd.concat([self.df_1, self.df_2], ignore_index=True)\n",
    "        self.df['original_line'] = self.df.applymap(lambda x: self.data_extract(x))\n",
    "        self.df['people_name'] = self.df[['original_line']].applymap(lambda x:str(x[0]) if x[0] is not '' else np.nan)\n",
    "        self.df['home_address'] = self.df[['original_line']].applymap(lambda x:str(x[1]) if x[1] is not None else np.nan)\n",
    "        self.df['occupation'] = self.df[['original_line']].applymap(lambda x:str(x[2]) if x[2] is not None else np.nan)\n",
    "        self.df['work_address'] = self.df[['original_line']].applymap(lambda x:str(x[3]) if x[3] is not None else np.nan)\n",
    "        ## drop na\n",
    "        self.df.dropna(subset=['people_name', 'home_address', 'occupation', 'work_address'], how='all', inplace = True)\n",
    "        return\n",
    "    ## imput image and number of columns:\n",
    "    def image_crop(self, num_col, imageObject):\n",
    "        \"\"\"Crop the image into halves\"\"\"\n",
    "        if num_col == 2:\n",
    "            width, height = imageObject.size\n",
    "            imageWithEdges = imageObject.filter(ImageFilter.FIND_EDGES)\n",
    "            imageWithEdges = np.asanyarray(imageWithEdges)\n",
    "\n",
    "            edges = canny(imageWithEdges, sigma=0.3, low_threshold=10, high_threshold=80)\n",
    "            lines = tf.probabilistic_hough_line(edges, threshold=10, line_length=80,\n",
    "                                             line_gap=3)\n",
    "            valid_width, valid_height = [], []   \n",
    "            ## just incase we don't have any thing from below methods.\n",
    "            up_conner = 0\n",
    "            for line in lines:\n",
    "                #  lines in format ((x0, y0), (x1, y1))\n",
    "                # width, height\n",
    "                p0, p1 = line\n",
    "                ## only want lines that are in the area of interest\n",
    "                if 2/5 * width < p0[0] < 3/5 * width and 2/5 * width < p1[0] < 3/5 * width and abs(p0[0] - p1[0]) < 1:\n",
    "                    valid_width.append(p0[0])\n",
    "                    valid_width.append(p1[0])\n",
    "            ## get the median number and consider the median as the line we want\n",
    "            if len(valid_width) == 0:\n",
    "                mid_left, mid_right = 895.0, 895.0\n",
    "            else:\n",
    "                mid_left = np.median(valid_width)\n",
    "                mid_right = np.median(valid_width) \n",
    "            ## get the top line from image processing method is unreliable\n",
    "            ## since the top line is too week to be detected. \n",
    "            ## we use OCR to get the top line\n",
    "            up_line = self.topCrop(imageObject)\n",
    "            ## crop the image\n",
    "            half1 = imageObject.crop((up_conner, up_line, mid_left, height))\n",
    "            half2 = imageObject.crop((mid_right, up_line, width, height))\n",
    "            return half1, half2\n",
    "\n",
    "    def preprocessImage(self, image):\n",
    "        \"\"\"Processes each column image to make it black and white\"\"\"\n",
    "        thresh = 150\n",
    "        fn = lambda x : 255 if x > thresh else 0\n",
    "\n",
    "        blackNwhite = image.convert('L').point(fn, mode='1')\n",
    "        sharpness = ImageEnhance.Sharpness(blackNwhite)\n",
    "        contrast = ImageEnhance.Contrast(sharpness.image)\n",
    "        brightness = ImageEnhance.Brightness(contrast.image)\n",
    "        enhancedImage = brightness.image\n",
    "        return enhancedImage\n",
    "\n",
    "    def topCrop(self, pageImage):\n",
    "        \"\"\"return the upper_line pixel from OCR extracted data\"\"\"\n",
    "        # Run OCR and extract data\n",
    "\n",
    "        directoryData = pytesseract.image_to_data(pageImage, output_type='data.frame')\n",
    "        directoryData = directoryData.dropna().reset_index()\n",
    "        directoryData['right'] = directoryData['left'] + directoryData['width']\n",
    "        directoryData['bottom'] = directoryData['top'] + directoryData['height']\n",
    "        # Label each line in dataframe\n",
    "\n",
    "        line = []\n",
    "        lineCount = 1\n",
    "\n",
    "        for r in range(len(directoryData)-1):\n",
    "            if directoryData['left'][r] < directoryData['left'][r+1]:\n",
    "                line.append(lineCount)\n",
    "            else:\n",
    "                line.append(lineCount)\n",
    "                lineCount += 1\n",
    "\n",
    "        line.append(lineCount)\n",
    "        directoryData['line'] = line\n",
    "\n",
    "        # Group by line to create Line Data dataframe\n",
    "\n",
    "        lineDataLT = directoryData[['line', 'left', 'top']].groupby('line').min().reset_index()\n",
    "        lineDataRB = directoryData[['line', 'right', 'bottom']].groupby('line').max().reset_index()\n",
    "        lineData = pd.merge(lineDataLT, lineDataRB)\n",
    "\n",
    "        width, height = pageImage.size\n",
    "        top = lineData['bottom'][0]+ 20\n",
    "        ## return the top pixel\n",
    "        return top\n",
    "    \n",
    "    def sideCrop(self, pageImage):\n",
    "        \"\"\"Crop the left and right margin\"\"\"\n",
    "        # Run OCR and extract data\n",
    "\n",
    "        directoryData = pytesseract.image_to_data(pageImage, output_type='data.frame')\n",
    "        directoryData = directoryData.dropna().reset_index()\n",
    "        directoryData['right'] = directoryData['left'] + directoryData['width']\n",
    "        directoryData['bottom'] = directoryData['top'] + directoryData['height']\n",
    "        # Label each line in dataframe\n",
    "        line = []\n",
    "        lineCount = 1\n",
    "\n",
    "        for r in range(len(directoryData)-1):\n",
    "            if directoryData['left'][r] < directoryData['left'][r+1]:\n",
    "                line.append(lineCount)\n",
    "            else:\n",
    "                line.append(lineCount)\n",
    "                lineCount += 1\n",
    "\n",
    "        line.append(lineCount)\n",
    "        directoryData['line'] = line\n",
    "\n",
    "        # Group by line to create Line Data dataframe\n",
    "\n",
    "        lineDataLT = directoryData[['line', 'left', 'top']].groupby('line').min().reset_index()\n",
    "        lineDataRB = directoryData[['line', 'right', 'bottom']].groupby('line').max().reset_index()\n",
    "        lineData = pd.merge(lineDataLT, lineDataRB)\n",
    "\n",
    "        width, height = pageImage.size\n",
    "        left = lineData['left'].mode()[0] - 10\n",
    "        right = lineData['right'].max()+ 3\n",
    "        cropImage = pageImage.crop((left, 0, width, height))\n",
    "        \n",
    "        return cropImage\n",
    "\n",
    "    def indentList(self, df):\n",
    "        \"\"\"Identify indented lines and develop column of binary identification as to whether a line is indented\"\"\"\n",
    "        indentColumn = []\n",
    "        for line in range(len(df)):\n",
    "            if df['left'][line] > (df['left'].mean()*3):\n",
    "                indentColumn.append(1)\n",
    "            else:\n",
    "                indentColumn.append(0)\n",
    "\n",
    "        # Pull list of rows that are indented, in reverse order\n",
    "\n",
    "        indentLineData = df[df['left'] > df['left'].mean()*3] # this mean metric might need tinkering\n",
    "        indentList = indentLineData['line'].tolist()\n",
    "        indentList = sorted(indentList, reverse=True)  \n",
    "\n",
    "        return indentList, indentColumn\n",
    "\n",
    "\n",
    "    def ocrInfo(self, pageImage):\n",
    "\n",
    "        \"\"\"Run OCR to extract word by word locative information (directoryData) and line specific data (lineData)\"\"\"\n",
    "\n",
    "        directoryData = pytesseract.image_to_data(pageImage, output_type='data.frame')\n",
    "        directoryData = directoryData.dropna().reset_index()\n",
    "        # directoryData = directoryData.drop([0,12])\n",
    "        directoryData = directoryData.dropna().reset_index()\n",
    "        directoryData['right'] = directoryData['left'] + directoryData['width']\n",
    "        directoryData['bottom'] = directoryData['top'] + directoryData['height']\n",
    "        line = []\n",
    "        lineCount = 1\n",
    "\n",
    "        for r in range(len(directoryData)-1):\n",
    "            if directoryData['left'][r] < directoryData['left'][r+1]:\n",
    "                line.append(lineCount)\n",
    "            else:\n",
    "                line.append(lineCount)\n",
    "                lineCount += 1\n",
    "\n",
    "        line.append(lineCount)\n",
    "        directoryData['line'] = line\n",
    "\n",
    "        lineDataLT = directoryData[['line', 'left', 'top']].groupby('line').min().reset_index()\n",
    "        lineDataRB = directoryData[['line', 'right', 'bottom']].groupby('line').max().reset_index()\n",
    "        lineData = pd.merge(lineDataLT, lineDataRB)\n",
    "\n",
    "        return lineData, directoryData\n",
    "    \n",
    "    \n",
    "    def data_extract(self, line):\n",
    "        \"\"\"extract people_name, home_address, occupation and work_address from the OCR extracted lines\"\"\"\n",
    "        # home address\n",
    "        people_name, home_address, occupation, work_address ='', None, None, None\n",
    "        # change the type of the line into a string\n",
    "        line= str(line)\n",
    "        # clean the string, remove special characters\n",
    "        reg = re.compile( \"[A-Za-z0-9 ,\\.]*\")\n",
    "        line = ''.join(reg.findall(line))\n",
    "        # line = re.sub('[wid\\.]', 'widow', line)\n",
    "        # if line is just space (this is very common in the OCR extracted original data)\n",
    "        if line.strip() == '':\n",
    "            return people_name, home_address, occupation, work_address\n",
    "\n",
    "        #if line has home address\n",
    "        if 'h.' in line:\n",
    "            home_address = line.split('h.')[1].strip()\n",
    "\n",
    "        line = line.split('h.')[0].strip()\n",
    "        ## people's name starts with upper case\n",
    "        count = 0\n",
    "        for word in str(line).split():\n",
    "            if word[0].isupper():\n",
    "                people_name = people_name + ' ' + word\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        line = ''.join(str(line).split()[count:])\n",
    "        line = re.split(',|\\.', line)\n",
    "\n",
    "        ## occupation should not start with number:\n",
    "    #     print(line)\n",
    "        if len(line) > 0 and len(line[0]) > 0 and line[0][0].islower():\n",
    "            occupation = line[0]\n",
    "            work_address = ' '.join(line[1:])\n",
    "        else:\n",
    "            work_address = ' '.join(line) \n",
    "\n",
    "        return people_name, home_address, occupation, work_address\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the name of the directories that each page has two_columns.\n",
    "\n",
    "\n",
    "two_columns = ['4ad95a70-317a-0134-d1af-00505686a51c','4adf9ec0-317a-0134-03ad-00505686a51c',\n",
    "                '4ae3cb40-317a-0134-489d-00505686a51c','4ae76b60-317a-0134-b849-00505686a51c',\n",
    "               '4aea8af0-317a-0134-2393-00505686a51c','4aed8a80-317a-0134-28a4-00505686a51c',\n",
    "               '4af0c6f0-317a-0134-e90c-00505686a51c','4af3b880-317a-0134-bda8-00505686a51c',\n",
    "               '4af6a690-317a-0134-5947-00505686a51c','4afa0510-317a-0134-cf84-00505686a51c',\n",
    "               '4afd6280-317a-0134-575a-00505686a51c','4b00bf60-317a-0134-32d0-00505686a51c',\n",
    "               '4b0419c0-317a-0134-7464-00505686a51c','4b073d20-317a-0134-af68-00505686a51c',\n",
    "               '4b0aa870-317a-0134-712b-00505686a51c','4b0e13f0-317a-0134-6578-00505686a51c',\n",
    "               '4b119360-317a-0134-9131-00505686a51c','4b154340-317a-0134-afd3-00505686a51c',\n",
    "                '4b18f080-317a-0134-fded-00505686a51c','4b336e60-317a-0134-1e9b-00505686a51c',\n",
    "                '4b36edd0-317a-0134-eedc-00505686a51c','4b3a14d0-317a-0134-011c-00505686a51c',\n",
    "               '4b3d0590-317a-0134-1631-00505686a51c','4b4009d0-317a-0134-949b-00505686a51c',\n",
    "               '4b437600-317a-0134-6db3-00505686a51c','4b47b740-317a-0134-ad0b-00505686a51c',\n",
    "               '4b4b2b90-317a-0134-6800-00505686a51c','4b4e8300-317a-0134-fb8c-00505686a51c',\n",
    "                '4b51d420-317a-0134-aa50-00505686a51c','4b5532f0-317a-0134-52ca-00505686a51c',\n",
    "                '4b58d200-317a-0134-d2aa-00505686a51c','4b5c40e0-317a-0134-e9c9-00505686a51c',\n",
    "                '4b5ff0e0-317a-0134-7e27-00505686a51c','4b63a460-317a-0134-d3bd-00505686a51c',\n",
    "                '4b66b460-317a-0134-8cb2-00505686a51c','4b69a410-317a-0134-a570-00505686a51c',\n",
    "                '4b6c95d0-317a-0134-f4e4-00505686a51c','4b6f8210-317a-0134-ff86-00505686a51c',\n",
    "                '4b728f10-317a-0134-8c07-00505686a51c','4b8e3f70-317a-0134-721a-00505686a51c',\n",
    "                '4b939190-317a-0134-d1d5-00505686a51c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "start_year, end_year = 1848, 1889\n",
    "for directory_name in two_columns:\n",
    "    start_year = start_year + 1\n",
    "    ## this is the path to the files in HPC\n",
    "    file_list = os.fsencode('/beegfs/nmw2-share/jpg/' + directory_name)\n",
    "    try:\n",
    "        \n",
    "        # some pages will throw an error. In order to prevent those pages\n",
    "        # from impacting the overall extraction we have built in \n",
    "        # a skip-functionality that will move past the error prone\n",
    "        # images.  \n",
    "        \n",
    "        for file in os.listdir(file_list):\n",
    "            filename = os.fsdecode(file)\n",
    "            imageObject = Image.open('/beegfs/nmw2-share/jpg/' + directory_name + '/' + filename)\n",
    "            new_df = Image_preprocessing(imageObject,2).df\n",
    "            new_df['year'] = start_year\n",
    "            ## the page_number is extracted from the name of the file.\n",
    "            ## note this is not the page number on the page.\n",
    "            new_df['page'] = filename.split('.')[0]\n",
    "            df = pd.concat([df, new_df], ignore_index=True)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resulting directories:\n",
    "df.to_csv('two_columns.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PUI2016_Python3",
   "language": "python",
   "name": "pui2016_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
